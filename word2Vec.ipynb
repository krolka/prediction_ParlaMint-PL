{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFSOGmPZd3XM",
        "outputId": "d42f055e-0aa4-4d87-a7ad-759af301ab51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pl_core_news_lg')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import re\n",
        "spacy.cli.download('pl_core_news_lg')\n",
        "nlp = spacy.load('pl_core_news_lg')\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.phrases import Phraser, Phrases\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, explained_variance_score, max_error, mean_absolute_error, mean_squared_error, r2_score, roc_curve, roc_auc_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC,  LinearSVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer(data):\n",
        "  tokenized = []\n",
        "  for s in data:\n",
        "    text = re.sub(r'\\[\\[.*?\\]\\]', '', s.lower())\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    tokenized.append(text.split(' '))\n",
        "  return tokenized\n",
        "\n",
        "def get_embeddings(x_test, x_train):\n",
        "\n",
        "  #training the model\n",
        "  model = gensim.models.Word2Vec(x_train+x_test, min_count = 1, vector_size = 100, window = 3)\n",
        "\n",
        "  print('lekarz', model.wv.most_similar('lekarz'))\n",
        "  print('podatek',model.wv.most_similar('podatek'))\n",
        "  print('szkoła',model.wv.most_similar('szkoła'))\n",
        "  print('kościół',model.wv.most_similar('kościół'))\n",
        "  print('gospodarka',model.wv.most_similar('gospodarka'))\n",
        "  print('premier',model.wv.most_similar('premier'))\n",
        "  # single word embeddings\n",
        "\n",
        "  train_embeddings = []\n",
        "  for speech in x_train:\n",
        "    embs = [model.wv.get_vector(word) for word in speech]\n",
        "    train_embeddings.append(embs)\n",
        "\n",
        "  #merging the embeddings into one vector\n",
        "  train_whole_embeddings = []\n",
        "  for feature in train_embeddings:\n",
        "    train_whole_embeddings.append([y for x in feature for y in x])\n",
        "\n",
        "  #test\n",
        "  test_embeddings = []\n",
        "  for speech in x_test:\n",
        "    embs = [model.wv.get_vector(word) for word in speech]\n",
        "    test_embeddings.append(embs)\n",
        "\n",
        "  #merging the embeddings into one vector\n",
        "  test_whole_embeddings = []\n",
        "  for feature in test_embeddings:\n",
        "    test_whole_embeddings.append([y for x in feature for y in x])\n",
        "\n",
        "  max_len_train = [max(len(lst) for lst in train_whole_embeddings)]\n",
        "  max_len_test = [max(len(lst) for lst in test_whole_embeddings)]\n",
        "\n",
        "  if max_len_train > max_len_test:\n",
        "    max_len = max_len_train\n",
        "  else:\n",
        "   max_len = max_len_test\n",
        "\n",
        "  train_padded_embeddings = [np.pad(np.array(lst), (0, max_len[0] - len(lst)), 'constant') for lst in train_whole_embeddings]\n",
        "  #vstack\n",
        "  train_padded_embeddings = np.vstack(train_padded_embeddings)\n",
        "  #padding\n",
        "  test_padded_embeddings = [np.pad(np.array(lst), (0, max_len[0] - len(lst)), 'constant') for lst in test_whole_embeddings]\n",
        "  #vstack\n",
        "  test_padded_embeddings = np.vstack(test_padded_embeddings)\n",
        "\n",
        "  return train_padded_embeddings, test_padded_embeddings"
      ],
      "metadata": {
        "id": "j5x2E7pQ6ff1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JwQ0y8z8czt"
      },
      "source": [
        "# loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmdeH2b3XMO0",
        "outputId": "3d126e54-d7e2-45f9-d971-c4dbf812d47f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zzHI3u9DjMb"
      },
      "outputs": [],
      "source": [
        "path = './drive/MyDrive/Master/thesis_codes/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iMpHM4SDjMb"
      },
      "outputs": [],
      "source": [
        "data_sejm = pd.read_csv(path+'data_sejm_spacy.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = np.load(path+'tokens_sejm_spacy.npy', allow_pickle = True)\n",
        "lemmas = np.load(path+'lemmas_sejm_spacy.npy', allow_pickle = True)"
      ],
      "metadata": {
        "id": "pa9D-25G7NxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_sejm['tokens'] = tokens\n",
        "data_sejm['lemmas'] = lemmas"
      ],
      "metadata": {
        "id": "J3hzy5iy7rXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TxSo_syA0hR"
      },
      "outputs": [],
      "source": [
        "data_sejm = data_sejm.dropna(subset=['Speaker_party'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUGaF95nkcpv"
      },
      "outputs": [],
      "source": [
        "data_sejm  = data_sejm [data_sejm['Speaker_party'].isin(['KO','PiS','Konfederacja','Lewica'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AN4v-d_niyix"
      },
      "outputs": [],
      "source": [
        "lengths = []\n",
        "for speech in data_sejm['tokens']:\n",
        "    lengths.append(len(speech))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RqooDZMkyq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e13ed90-940e-49e5-cfdb-e659a27e6454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-57-531b2a4ba98f>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_sejm['lengths'] = lengths\n"
          ]
        }
      ],
      "source": [
        "data_sejm['lengths'] = lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BopQlw5ek4ra"
      },
      "outputs": [],
      "source": [
        "data_sejm = data_sejm[data_sejm['lengths'] < 1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3xFXKq5DjMb"
      },
      "outputs": [],
      "source": [
        "sample_data = data_sejm.sample(n = 10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wS_njqlPf4E"
      },
      "outputs": [],
      "source": [
        "sample_data = sample_data.sample(frac=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QlAXY9dIW1W"
      },
      "outputs": [],
      "source": [
        "party_numbers = []\n",
        "for p in sample_data['Speaker_party']:\n",
        "  if p == 'KO':\n",
        "    party_numbers.append(0)\n",
        "  if p == 'PiS':\n",
        "    party_numbers.append(1)\n",
        "  if p == 'Konfederacja':\n",
        "    party_numbers.append(2)\n",
        "  if p == 'Lewica':\n",
        "    party_numbers.append(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mb0rewy3IcJ9"
      },
      "outputs": [],
      "source": [
        "sample_data['Party_tag'] = party_numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvveBrmoIWY-"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(sample_data['processed_speech'], sample_data['Party_tag'], test_size=0.2, stratify = sample_data['Party_tag']  ,random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACoOhwgilGCZ"
      },
      "source": [
        "# Calculating embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MabxY4uieskh"
      },
      "outputs": [],
      "source": [
        "x_train = tokenizer(X_train)\n",
        "x_test = tokenizer(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBc82najR5Ki"
      },
      "outputs": [],
      "source": [
        "X_train, X_test = get_embeddings(x_test, x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2lraTyzbV_8"
      },
      "source": [
        "# Classifications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZttJZd3EDov"
      },
      "source": [
        "PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzOHNsGBjmPr",
        "outputId": "6d03491c-696a-4c07-b95a-d13c944e275d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9\n"
          ]
        }
      ],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "pca = PCA(n_components = 0.9, copy = False)\n",
        "print(pca.n_components)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPpqwvaEwDcC"
      },
      "outputs": [],
      "source": [
        "del pca, X_train, X_test, scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SO6gfQEVlNjI"
      },
      "outputs": [],
      "source": [
        "def compare_models(X_train, X_test, y_train,y_test, cm_name):\n",
        "\n",
        "  class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "  class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "  print(class_weight_dict)\n",
        "\n",
        "  scaler = MinMaxScaler()\n",
        "\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "  best_classifiers = {}\n",
        "\n",
        "  #svc\n",
        "  svc = SVC(class_weight = class_weight_dict, decision_function_shape='ovo', max_iter = 100000)\n",
        "  svc = svc.fit(X_train, y_train)\n",
        "  svc_pred = svc.predict(X_test)\n",
        "  print('svc classification results')\n",
        "  print(classification_report(y_test, svc_pred, zero_division = 1))\n",
        "  best_classifiers['svc'] = svc_pred\n",
        "\n",
        "  print('_'*200)\n",
        "\n",
        "  #nb\n",
        "  nb = GaussianNB()\n",
        "  nb.class_prior_ = np.array(list(class_weight_dict.values())) / sum(class_weight_dict.values())\n",
        "  nb = nb.fit(X_train, y_train)\n",
        "  nb_pred = nb.predict(X_test)\n",
        "  print('NB classification results')\n",
        "  print(classification_report(y_test, nb_pred, zero_division = 1))\n",
        "  print('_'*200)\n",
        "  best_classifiers['nb'] = nb_pred\n",
        "\n",
        "  mnb = MultinomialNB(force_alpha=True)\n",
        "  mnb.fit(X_train, y_train)\n",
        "  mnb_pred = mnb.predict(X_test)\n",
        "  print('MNB classification results')\n",
        "  print(classification_report(y_test, mnb_pred, zero_division = 1))\n",
        "  print('_'*200)\n",
        "  best_classifiers['mnb'] = mnb_pred\n",
        "\n",
        "  #random forest\n",
        "  rf = RandomForestClassifier(class_weight = class_weight_dict)\n",
        "  rf = rf.fit(X_train, y_train)\n",
        "  rf_pred = rf.predict(X_test)\n",
        "  print('RF classification results')\n",
        "  print(classification_report(y_test, rf_pred))\n",
        "  print('_'*200)\n",
        "  best_classifiers['rf'] = rf_pred\n",
        "\n",
        "\n",
        "\n",
        "  #multilayer\n",
        "  mlp = MLPClassifier(alpha = 0.05, learning_rate = 'adaptive')\n",
        "  mlp.class_prior_ = np.array(list(class_weight_dict.values())) / sum(class_weight_dict.values())\n",
        "  mlp = mlp.fit(X_train, y_train)\n",
        "  mlp_pred = mlp.predict(X_test)\n",
        "  print('mlp classification results')\n",
        "  print(classification_report(y_test, mlp_pred, zero_division = 1))\n",
        "  print('_'*200)\n",
        "  best_classifiers['mlp'] = mlp_pred\n",
        "\n",
        "  #dt\n",
        "  dt = DecisionTreeClassifier(random_state = 0, max_depth = 7, class_weight = class_weight_dict)\n",
        "  dt = dt.fit(X_train, y_train)\n",
        "  dt_pred = dt.predict(X_test)\n",
        "  print('dt classification results')\n",
        "  print(classification_report(y_test, dt_pred, zero_division = 1))\n",
        "  print('_'*200)\n",
        "  best_classifiers['dt'] = dt_pred\n",
        "\n",
        "\n",
        "  #sb\n",
        "  ab = AdaBoostClassifier()\n",
        "  ab.class_prior_ = np.array(list(class_weight_dict.values())) / sum(class_weight_dict.values())\n",
        "  ab = ab.fit(X_train, y_train)\n",
        "  ab_pred = ab.predict(X_test)\n",
        "  print('AB classification results')\n",
        "  print(classification_report(y_test, ab_pred, zero_division = 1))\n",
        "  print('_'*200)\n",
        "  best_classifiers['ab'] = ab_pred\n",
        "\n",
        "\n",
        "  #logistic regression\n",
        "  reg = LogisticRegression(max_iter = 10000, class_weight = class_weight_dict, multi_class = 'multinomial')\n",
        "  reg = reg.fit(X_train, y_train)\n",
        "  reg_pred = reg.predict(X_test)\n",
        "  best_classifiers['reg'] = reg_pred\n",
        "\n",
        "  print('REG classification results')\n",
        "  print(classification_report(y_test, reg_pred, zero_division = 1))\n",
        "  print('_'*200)\n",
        "\n",
        "  #knn\n",
        "  neigh = KNeighborsClassifier(n_neighbors = 4)\n",
        "  neigh.class_prior_ = np.array(list(class_weight_dict.values())) / sum(class_weight_dict.values())\n",
        "  neigh = neigh.fit(X_train_pca, y_train)\n",
        "  knn_pred = neigh.predict(X_test_pca)\n",
        "\n",
        "  best_classifiers['knn'] = knn_pred\n",
        "\n",
        "  print('KNN classification results')\n",
        "  print(classification_report(y_test, knn_pred, zero_division = 1))\n",
        "  print('_'*200)\n",
        "\n",
        "  best_classifier = input('pick best classifier: ')\n",
        "\n",
        "  cm = confusion_matrix(y_test, best_classifiers[best_classifier])\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "  plt.figure(figsize = [10,10])\n",
        "  ax = sns.heatmap(cm, annot = True, fmt = 'd', annot_kws = {'size': 20} , cmap = sns.color_palette(\"Blues\", as_cmap=True))\n",
        "  ax.xaxis.set_ticklabels(['KO', 'PiS', 'Konfederacja', 'Lewica'], fontsize = 20)\n",
        "  ax.yaxis.set_ticklabels(['KO', 'PiS', 'Konfederacja', 'Lewica'], fontsize = 20)\n",
        "  ax.set_xlabel('Predicted label', fontsize = 20)\n",
        "  ax.set_ylabel('True label', fontsize = 20)\n",
        "  plt.savefig(cm_name)\n",
        "  plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hf-pnP-UmM7Z"
      },
      "outputs": [],
      "source": [
        "compare_models(X_train_pca, X_test_pca, y_train,y_test, 'spacy_party_w2v_sejm.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx2oC9n7rvqo"
      },
      "source": [
        "# For balanced dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvs4sK3aryRI"
      },
      "outputs": [],
      "source": [
        "data_pis = data_sejm.loc[data_sejm['Speaker_party'] == 'PiS'].sample(n = 2000)\n",
        "data_ko = data_sejm.loc[data_sejm['Speaker_party'] == 'KO'].sample(n = 2000)\n",
        "data_konf = data_sejm.loc[data_sejm['Speaker_party'] == 'Konfederacja'].sample(n = 2000)\n",
        "data_lew = data_sejm.loc[data_sejm['Speaker_party'] == 'Lewica'].sample(n = 2000)\n",
        "sample_data = pd.concat([data_pis, data_ko, data_konf, data_lew], ignore_index = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qDqcuG7ryRK"
      },
      "outputs": [],
      "source": [
        "sample_data = sample_data.sample(frac=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_P2k3B2ryRM"
      },
      "outputs": [],
      "source": [
        "party_numbers = []\n",
        "for p in sample_data['Speaker_party']:\n",
        "  if p == 'KO':\n",
        "    party_numbers.append(0)\n",
        "  if p == 'PiS':\n",
        "    party_numbers.append(1)\n",
        "  if p == 'Konfederacja':\n",
        "    party_numbers.append(2)\n",
        "  if p == 'Lewica':\n",
        "    party_numbers.append(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_w6Va5tryRN"
      },
      "outputs": [],
      "source": [
        "sample_data['Party_tag'] = party_numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1o9AeJRryRN"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(sample_data['processed_speech'], sample_data['Party_tag'], test_size=0.2, stratify = sample_data['Party_tag']  ,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFxtufvu7Kja"
      },
      "outputs": [],
      "source": [
        "x_train = tokenizer(X_train)\n",
        "x_test = tokenizer(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ji9mrMGY7Kjc"
      },
      "outputs": [],
      "source": [
        "X_train, X_test = get_embeddings(x_test, x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zN4cQiZryRR",
        "outputId": "c169f0e6-08ec-4776-ede7-b2a6fcc6d5be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9\n"
          ]
        }
      ],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "pca = PCA(n_components = 0.9, copy = False)\n",
        "print(pca.n_components)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWw6vzvpryRT"
      },
      "outputs": [],
      "source": [
        "compare_models(X_train_pca, X_test_pca, y_train,y_test, 'spacy_party_w2v_sejm_balanced.jpg')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}